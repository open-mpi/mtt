#
# Copyright (c) 2006-2009 Cisco Systems, Inc.  All rights reserved.
# Copyright (c) 2006-2007 Sun Microystems, Inc.  All rights reserved.
# Copyright (c) 2008      High Performance Computing Center Stuttgart,
#                         University of Stuttgart.  All rights reserved.
#

# Template MTT configuration file for Open MPI core testers.  Note
# that there are many items in this file that, while they are good for
# examples, may not work for random MTT users.  For example, the
# "ompi-tests" Git repository that is used in many of the examples
# below is *not* a public repository (there's nothing really secret in
# this repository; it contains many publicly-available MPI tests and
# benchmarks, but we have never looked into the redistribution rights
# of these codes, so we keep the Git repository "closed" to the
# general public and only use it internally in the Open MPI project).

#-------------------------------------------------------------------------

# The intent for this template file is to establish at least some
# loose guidelines for what Open MPI core testers should be running on
# a nightly basis.  This file is not intended to be an exhaustive
# sample of all possible fields and values that MTT offers.  Each site
# will undoubtedly have to edit this template for their local needs
# (e.g., pick compilers to use, etc.), but this file provides a
# baseline set of configurations that we intend you to run.

# OMPI core members will need to edit some values in this file based
# on your local testing environment.  Look for comments with "OMPI
# Core:" for instructions on what to change.

# Note that this file is artificially longer than it really needs to
# be -- a bunch of values are explicitly set here that are exactly
# equivalent to their defaults.  This is mainly because there is no
# reliable form of documentation for this ini file yet, so the values
# here comprise a good set of what options are settable (although it
# is not a comprehensive set).

# Also keep in mind that at the time of this writing, MTT is still
# under active development and therefore the baselines established in
# this file may change on a relatively frequent basis.

# The guidelines are as follows:
#
# 1. Download and test nightly snapshot tarballs of at least one of
#    the following:
#    - the master (highest preference)
#    - release branches (highest preference is the most recent release
#      branch; lowest preference is the oldest release branch)
# 2. Run all 4 correctness test suites from the ompi-tests Git repo
#    - trivial, as many processes as possible
#    - intel tests with all_tests_no_perf, up to 64 processes
#    - IBM, as many processes as possible
#    - IMB, as many processes as possible
# 3. Run with as many different components as possible
#    - PMLs (ob1, dr)
#    - BTLs (iterate through sm, tcp, whatever high speed network(s) you
#      have, etc. -- as relevant)

#======================================================================
# Overall configuration
#======================================================================

[MTT]

# OMPI Core: if you are not running in a scheduled environment and you
# have a fixed hostfile for what nodes you'll be running on, fill in
# the absolute pathname to it here.  If you do not have a hostfile,
# leave it empty.  Example:
#     hostfile = /home/me/mtt-runs/mtt-hostfile
# This file will be parsed and will automatically set a valid value
# for &env_max_np() (it'll count the number of lines in the hostfile,
# adding slots/cpu counts if it finds them).  The "hostfile" value is
# ignored if you are running in a recognized scheduled environment.
hostfile =

# OMPI Core: if you would rather list the hosts individually on the
# mpirun command line, list hosts here delimited by whitespace (if you
# have a hostfile listed above, this value will be ignored!).  Hosts
# can optionally be suffixed with ":num", where "num" is an integer
# indicating how many processes may be started on that machine (if not
# specified, ":1" is assumed).  The sum of all of these values is used
# for &env_max_np() at run time.  Example (4 uniprocessors):
#    hostlist = node1 node2 node3 node4
# Another example (4 2-way SMPs):
#    hostlist = node1:2 node2:2 node3:2 node4:2
# The "hostlist" value is ignored if you are running in a scheduled
# environment or if you have specified a hostfile.
hostlist =

# OMPI Core: if you are running in a scheduled environment and want to
# override the scheduler and set the maximum number of processes
# returned by &env_max_procs(), you can fill in an integer here.
max_np = 

# OMPI Core: Output display preference; the default width at which MTT
# output will wrap.
textwrap = 76

# OMPI Core: After the timeout for a command has passed, wait this
# many additional seconds to drain all output, and then kill it with
# extreme prejiduce.
drain_timeout = 5

# OMPI Core: Whether this invocation of the client is a test of the
# client setup itself.  Specifically, this value should be set to true
# (1) if you are testing your MTT client and/or INI file and do not
# want the results included in normal reporting in the MTT central
# results database.  Results submitted in "trial" mode are not
# viewable (by default) on the central database, and are automatically
# deleted from the database after a short time period (e.g., a week).
# Setting this value to 1 is exactly equivalent to passing "--trial"
# on the MTT client command line.  However, any value specified here
# in this INI file will override the "--trial" setting on the command
# line (i.e., if you set "trial = 0" here in the INI file, that will
# override and cancel the effect of "--trial" on the command line).
# trial = 0

# OMPI Core: Set the scratch parameter here (if you do not want it to
# be automatically set to your current working directory). Setting
# this parameter accomplishes the same thing that the --scratch option
# does.
# scratch = &getenv("HOME")/mtt-scratch

# OMPI Core: Set local_username here if you would prefer to not have
# your local user ID in the MTT database
# local_username =

# OMPI Core: --force can be set here, instead of at the command line.
# Useful for a developer workspace in which it makes no sense to not
# use --force
# force = 1

# OMPI Core: Specify a list of sentinel files that MTT will regularly
# check for.  If these files exist, MTT will exit more-or-less
# immediately (i.e., after the current test completes) and report all
# of its results.  This is a graceful mechanism to make MTT stop right
# where it is but not lose any results.
# terminate_files = &getenv("HOME")/mtt-stop,&scratch_root()/mtt-stop

# OMPI Core: Specify a default description string that is used in the
# absence of description strings in the MPI install, Test build, and
# Test run sections.  The intent of this field is to record variable
# data that is outside the scope, but has effect on the software under
# test (e.g., firmware version of a NIC).  If no description string is
# specified here and no description strings are specified below, the
# description data field is left empty when reported.  
# description = NIC firmware: &system("get_nic_firmware_rev")

# OMPI Core: Specify a logfile where you want all MTT output to be
# sent in addition to stdout / stderr.
# logfile = /tmp/my-logfile.txt

# OMPI Core: If you have additional .pm files for your own funclets,
# you can have a comma-delimited list of them here.  Note that each
# .pm file *must* be a package within the MTT::Values::Functions
# namespace.  For example, having a Cisco.pm file must include the
# line:
#
#     package MTT::Values::Functions::Cisco;
#
# If this file contains a perl function named foo, you can invoke this
# functlet as &Cisco::foo().  Note that funclet files are loaded
# almost immediately, so you can use them even for other field values
# in the MTT section.
# funclet_files = /path/to/file1.pm, /path/to/file2.pm

# OMPI Core: To ensure that MTT doesn't fill up your disk, you can
# tell MTT to stop when disk space gets too low.  You can specify a
# raw number of bytes or a percentage of overall disk space.  For
# example (default value is "5%"):
#
# min_disk_free = 5% # stop when there's less than 5% disk free
# min_disk_free = 500000 # stop when there's less than 500,000 bytes free

# OMPI Core: When MTT detects a low-disk situation, it can wait a
# little while before reporting whatever results it has accumulated
# and exiting.  The min_disk_free_wait field specifies a number of
# minutes to wait for there to be enough disk space to be free.  If
# there is still not enough disk space at the end of that time, MTT
# will report accumulated results and quit.
#
# min_disk_free_wait = 60

#
# Submit results on per section basis as alterative to "submit_results_after_n_results"
#
submit_group_results = 1

# code to run on mtt start
# on_start=&shell("modprobe ummunot")

# code to run on mtt stop
# on_stop=&shell("modprobe -r ummunot")

#----------------------------------------------------------------------

[Lock]
# The only module available is the MTTLockServer, and requires running
# the mtt-lock-server executable somewhere.  You can leave this
# section blank and there will be no locking.
#module = MTTLockServer
#mttlockserver_host = hostname where mtt-lock-server is running
#mttlockserver_port = integer port number of the mtt-lock-server

#======================================================================
# MPI get phase
#======================================================================

[MPI get: ompi-nightly-master]
mpi_details = Open MPI

module = OMPI_Snapshot
ompi_snapshot_url = https://www.open-mpi.org/nightly/master

#----------------------------------------------------------------------

# OMPI Core: If you do not want to test nightly v1.8 tarballs, use the
# --no-section client command line flag or comment out this section.
[MPI get: ompi-nightly-v1.10]
mpi_details = Open MPI

module = OMPI_Snapshot
ompi_snapshot_url = https://www.open-mpi.org/nightly/v1.10

#----------------------------------------------------------------------

# OMPI Core: If you do not want to test nightly v2.0.x tarballs, use the
# --no-section client command line flag or comment out this section.
[MPI get: ompi-nightly-v2.0.x]
mpi_details = Open MPI
module = OMPI_Snapshot
ompi_snapshot_url = https://www.open-mpi.org/nightly/v2.x

#======================================================================
# Install MPI phase
#======================================================================

[MPI install: gcc warnings]
mpi_get = ompi-nightly-master,ompi-nightly-v2.0.x,ompi-nightly-v1.10
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = OMPI
ompi_vpath_mode = none
# OMPI Core: This is a GNU make option; if you are not using GNU make,
# you'll probably want to delete this field (i.e., leave it to its
# default [empty] value).
ompi_make_all_arguments = -j 32
ompi_make_check = 1
# OMPI Core: You will likely need to update these values for whatever
# compiler you want to use.  You can pass any configure flags that you
# want, including those that change which compiler to use (e.g., CC=cc
# CXX=CC F77=f77 FC=f90).  Valid compiler names are: gnu, pgi, intel,
# ibm, kai, absoft, pathscale, sun.  If you have other compiler names
# that you need, let us know.  Note that the compiler_name flag is
# solely for classifying test results; it does not automatically pass
# values to configure to set the compiler.
ompi_compiler_name = gnu
ompi_compiler_version = &get_gcc_version()
ompi_configure_arguments = CFLAGS=-pipe --enable-picky --enable-debug

#----------------------------------------------------------------------

# Similar to above, but using the intel compilers and using an
# environment module.
[MPI install: intel warnings]
mpi_get = ompi-nightly-master,ompi-nightly-v2.0.x,ompi-nightly-v1.10
save_stdout_on_success = 1
merge_stdout_stderr = 0
# Load the environment module named "intel-compilers/9.0" before
# executing this section (and any test phase section that uses this
# MPI install section).  This line is only suitable for those who use
# the environment modules package (http://modules.sourceforge.net/)
env_module = intel-compilers/2016-16.0.0.109

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 8
ompi_make_check = 1
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc F77=ifort FC=ifort CFLAGS=-g --enable-picky --enable-debug

# Sun MPI install section illustrating the use of
# $var style substitution and &perl()
[MPI install: sun-autotools]

configure_arguments = \
        $prepend_configure_arguments \
        $compiler_names \
        $compiler_flags \
        $append_configure_arguments \
        $with_mx_lib_argument \
        $with_tm_argument

# ompi-nightly-*
mpi_get =

# 32|64
bitness =

# --whatever, ...
prepend_configure_arguments = 

# --whatever, ...
append_configure_arguments =

# Files and directories
arch = &shell("uname -p")
home = &getenv("HOME")

mtt_utils_dir       = $home/mtt-utils
ompi_build_dir      = $home/ompi-tools/share/ompi-build
compiler_names      = CC=cc CXX=CC FC=f90 F77=f77
compiler_flags_file = $ompi_build_dir/comp-flags.sos.$arch.$bitness.opt
compiler_flags      = &shell("cat $compiler_flags_file")

# Only use this option if the MX directory exists 
mx_lib = /opt/mx/lib
with_mx_lib_argument = <<EOT
&perl("
    if (-d '$mx_lib') {
        return '--with-mx-lib=$mx_lib';
    } else {
        return '';
    }
")
EOT

# Only use this option if the OpenPBS directory exists 
tm = /hpc/rte/OpenPBS-$arch
with_tm_argument = <<EOT
&perl("
    if (-d '$tm') {
        return '--with-tm=$tm';
    } else {
        return '';
    }
")
EOT

# Other settings
save_stdout_on_success = 1
merge_stdout_stderr = 1
vpath_mode = none
make_all_arguments = -j 4
make_check = 0
compiler_name = sun
compiler_version = &get_sun_cc_version()
module = OMPI


# Other compiler version funclets that are available:
#    &get_pgcc_version : PGI compiler suite
#    &get_pathcc_version : Pathscale compiler suite
#    &get_sun_version : Sun compiler suite

#======================================================================
# MPI run details
#======================================================================

[MPI Details: Open MPI]

# MPI tests
exec = mpirun @hosts@ -np &test_np() @mca@ --prefix &test_prefix() &test_executable() &test_argv()

hosts = &if(&have_hostfile(), "--hostfile " . &hostfile(), \
            &if(&have_hostlist(), "--host " . &hostlist(), ""))

# Example showing conditional substitution based on the MPI get
# section name (e.g., different versions of OMPI have different
# capabilities / bugs).
mca = &enumerate( \
        "--mca btl vader,tcp,self @mca_params@", \
        "--mca btl tcp,self @mca_params@")

# MCA params that are suitable for your environment
mca_params = --mca btl_tcp_if_include eth0 --mca oob_tcp_if_include eth0

# A helper script is installed by the "OMPI" MPI Install module named
# "mtt_ompi_cleanup.pl".  This script is orterun-able and will kill
# all rogue orteds on a node and whack any session directories.
# Invoke via orterun just to emphasize that it is not an MPI
# application.  The helper script is installed in OMPI's bin dir, so
# it'll automatically be found in the path (because OMPI's bin dir is
# in the path).

after_each_exec = <<EOT
# We can exit if the test passed or was skipped (i.e., there's no need
# to cleanup).
if test "$MTT_TEST_RUN_RESULT" = "passed" -o "$MTT_TEST_RUN_RESULT" = "skipped"; then
    exit 0
fi

if test "$MTT_TEST_HOSTFILE" != ""; then
    args="--hostfile $MTT_TEST_HOSTFILE"
elif test "$MTT_TEST_HOSTLIST" != ""; then
    args="--host $MTT_TEST_HOSTLIST"
fi
orterun $args -np $MTT_TEST_NP --prefix $MTT_TEST_PREFIX mtt_ompi_cleanup.pl
EOT

#======================================================================
# Test get phase
#======================================================================

[Test get: trivial]
module = Trivial

#----------------------------------------------------------------------

[Test get: ibm]
module = SCM
scm_module = Git
scm_url = https://github.com/open-mpi/ompi-tests.git
scm_subdir = intel_tests
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: onesided]
module = SCM
scm_module = Git
scm_url = https://github.com/open-mpi/ompi-tests.git
scm_subdir = onesided
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: mpicxx]
module = SCM
scm_module = Git
scm_url = https://github.com/open-mpi/ompi-tests.git
scm_subdir = mpicxx
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: imb]
module = SCM
scm_module = Git
scm_url = https://github.com/open-mpi/ompi-tests.git
scm_subdir = imb

#----------------------------------------------------------------------

[Test get: netpipe]
module = SCM
scm_module = Git
scm_url = https://ompiteam-mtt:ompi4life@github.com/open-mpi/ompi-tests.git
scm_subdir = NetPIPE-3.7.1

#======================================================================
# Test build phase
#======================================================================

[Test build: trivial]
test_get = trivial
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Trivial

#----------------------------------------------------------------------

[Test build: ibm]
test_get = ibm
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
./configure --enable-static --disable-shared
make
EOT

#----------------------------------------------------------------------

[Test build: onesided]
test_get = onesided
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
./configure
make
EOT

#----------------------------------------------------------------------

[Test build: mpicxx]
test_get = mpicxx
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++
make
EOT

#----------------------------------------------------------------------

[Test build: imb]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1 IMB-EXT
EOT

#----------------------------------------------------------------------

[Test build: netpipe]
test_get = netpipe
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make mpi
EOT

#======================================================================
# Test Run phase
#======================================================================

# This section is not used directly; it is included in others.
[Defaults Test run]
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = &and(&test_wifexited(), &eq(&test_wexitstatus(), 77))

save_stdout_on_pass = 1
merge_stdout_stderr = 1
stdout_save_lines = 100
stderr_save_lines = 100
report_after_n_results = 100

np = &env_max_procs()

#----------------------------------------------------------------------

[Test run: trivial]
include_section = Defaults Test run

test_build = trivial
timeout = &max(10, &test_np())
skipped = 0

specify_module = Simple
simple_first:tests = &find_executables(".")

#----------------------------------------------------------------------

[Test run: ibm]
include_section = Defaults Test run

test_build = ibm
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_first:np = &env_max_procs()
simple_first:tests = &find_executables("collective", "communicator", \
                                       "datatype", "dynamic", "environment", \
                                       "group", "info", "io", "onesided", \
                                       "pt2pt", "random", "topology")

# Tests that are supposed to fail
simple_fail:tests = environment/abort environment/final
simple_fail:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_fail:exclusive = 1
simple_fail:timeout = &env_max_procs()

# Spawn tests; need to run very few
simple_spawns:tests  = dynamic/spawn dynamic/spawn_multiple
simple_spawns:np = 3
simple_spawns:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_spawns:exclusive = 1
simple_spawns:timeout = &multiply(5,&env_max_procs())

# Big loop o' spawns
simple_loop_spawn:tests = dynamic/loop_spawn
simple_loop_spawn:np = 1
simple_loop_spawn:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_loop_spawn:exclusive = 1
simple_loop_spawn:timeout = 600

# Big loop o' comm splits and whatnot.  It runs for 10 minutes.
simple_loop_comm_split:tests = communicator/comm_split_f
simple_loop_comm_split:np = 1
simple_loop_comm_split:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_loop_comm_split:exclusive = 1
simple_loop_comm_split:timeout = 660

# THREAD_MULTIPLE test will fail with the openib btl because it
# deactivates itself in the presence of THREAD_MULTIPLE.  So just skip
# it.  loop_child is the target for loop_spawn, so we don't need to
# run it (although it'll safely pass if you run it by itself).
simple_skip:tests = environment/init_thread_multiple dynamic/loop_child
simple_skip:exclusive = 1
simple_skip:do_not_run = 1

#----------------------------------------------------------------------

[Test run: onesided]
include_section = Defaults Test run

test_build = onesided
timeout = &max(30, &multiply(10, &test_np()))

np = &if(&gt(&env_max_procs(), 0), &step(2, &max(2, &env_max_procs()), 2), 2)

simple_pass:tests = &cat("run_list")

#----------------------------------------------------------------------

[Test run: mpicxx]
include_section = Defaults Test run

test_build = mpicxx
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_pass:tests = src/mpi2c++_test src/mpi2c++_dynamics_test

#----------------------------------------------------------------------
     
[Test run: imb-general]
include_section = Defaults Test run

test_build = imb-general
timeout = &max(2800, &multiply(50, &test_np()))
np = &min("32", &env_max_procs())

argv = -npmin &test_np()

specify_module = Simple
simple_pass:tests = src/IMB-MPI1 src/IMB-EXT

#----------------------------------------------------------------------

[Test run: imb-check]
include_section = Defaults Test run

test_build = imb-check
timeout = &max(2800, &multiply(50, &test_np()))
np = &min("32", &env_max_procs())

argv = -npmin &test_np()

specify_module = Simple
simple_pass:tests = src/IMB-MPI1 src/IMB-EXT

#----------------------------------------------------------------------

[Test run: imb performance]
include_section = Defaults Test run
test_build = imb

pass = &eq(&cmd_wexitstatus(), 0)
timeout = -1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1

argv = -npmin &test_np() &enumerate("PingPong", "PingPing", "Sendrecv", "Exchange", "Allreduce", "Reduce", "Reduce_scatter", "Allgather", "Allgatherv", "Alltoall", "Bcast", "Barrier") 

analyze_module = IMB
simple_pass:tests = src/IMB-MPI1

#----------------------------------------------------------------------

[Test run: netpipe-performance]
include_section = Defaults Test run

test_build = netpipe
skipped = 0
timeout = &multiply(&test_np(), 120)
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
np = 2

specify_module = Simple
analyze_module = NetPipe
simple_pass:tests = NPmpi

#----------------------------------------------------------------------

[Test run: nbcbench]
include_section = Defaults Test run
test_build = nbcbench

pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
timeout = -1
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
merge_stdout_stderr = 1

analyze_module = NBCBench
simple_pass:tests = nbcbench

argv = -p &test_np()-&test_np() -s 1-1048576 -v -t \
    &enumerate("MPI_Allgatherv", "MPI_Allgather", "MPI_Allreduce", \
    "MPI_Alltoall", "MPI_Alltoallv", "MPI_Barrier", "MPI_Bcast", \
    "MPI_Gather", "MPI_Gatherv", "MPI_Reduce", "MPI_Reduce_scatter", \
    "MPI_Scan", "MPI_Scatter", "MPI_Scatterv")

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_url = https://www.open-mpi.org/mtt/submit/
# OMPI Core: Change this to be the username and password for your
# submit user.  Get this from the OMPI MTT administrator.
mttdatabase_username = >>> you must set this value <<<
mttdatabase_password = >>> you must set this value <<<
# OMPI Core: Change this to be some short string identifying your
# cluster.
mttdatabase_platform = >>> you must set this value <<<

#----------------------------------------------------------------------
# The reporter below is in development - submissions are marked as 'trial'
# [Reporter: IU REST Storage]
# module = MTTStorage

# mttstorage_realm = OMPI
# mttstorage_url = https://www.open-mpi.org/mtt/submit/api
# # OMPI Core: Change this to be the username and password for your
# # submit user.  Get this from the OMPI MTT administrator.
# mttstorage_username = >>> you must set this value <<<
# mttstorage_password = >>> you must set this value <<<
# # OMPI Core: Change this to be some short string identifying your
# # cluster.
# mttstorage_platform = >>> you must set this value <<<

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file

[Reporter: text file backup]
module = TextFile

textfile_filename = $phase-$section-$mpi_name-$mpi_version.txt

textfile_summary_header = <<EOT
hostname: &shell("hostname")
uname: &shell("uname -a")
who am i: &shell("who am i")
EOT

textfile_summary_footer =
textfile_detail_header =
textfile_detail_footer =

textfile_textwrap = 78

# Send digested summary of mtt execution by email
#email_to = 
#email_subject = MTT test has completed, status: $overall_mtt_status
#email_footer = <<EOT
#Test Scratch Directory is &scratch_root()
#EOT

#----------------------------------------------------------------------

[Reporter: send email]
module = Email
email_to = fill this in
email_subject = MPI test results: &current_section()
